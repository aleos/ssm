\section{Структурно-стохастические модели (ССМ) функционирования БС}

Основная задача анализа ТМИ~--- получение значений параметров
\emph{цели анализа}.

При сборе ТМИ вследствие разного рода неучтённых и непредсказуемых
факторов, разрушающих информацию, возможны различные по природе
искажения значений параметров ТС $X_и$ (измеряемых). Это, в свою
очередь, может повлиять на определение значений вычисляемых параметров
$X_в$, и, как следствие, к неправильному оцениванию параметров цели
анализа $C$.

Поэтому в системах анализа ТМИ необходимо использовать модели
стохастического типа.

При использовании структурных (лингвистических) моделей
функционирования БС такими моделями являются \emph{стохастические
  грамматики (СГ), стохастические автоматы (СА), стохастические языки
  (СЯ)}, являющиеся \emph{структурно-стохастическими моделями
  функционирования БС}.

Перейдем к их рассмотрению.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Определение и основные типы структурно-стохастических моделей}

\subsubsection{Определение и классификация СГ}

Основным понятием при рассмотрении ССМ является понятие СГ. Дадим её
определение.

\begin{defin}
  Стохастическая грамматика (СГ) $G_s$~--- есть кортеж (восьмёрка):
  \begin{equation}
    \label{eq:Gs_general}
    G_s = \langle N, T, R, S, \Pi_S, \pi_S, \Pi_R, \pi_R \rangle\,,
  \end{equation}
  где\begin{tabular}[t]{r@{\;---\;}p{.88\textwidth}@{}}
    $N$ & конечное множество нетерминальных символов (алфавит
    нетерминалов), вспомогательных символов;\\
    
    $T$ & конечное множество терминальных символов (алфавит
    терминалов);\\
    
    $R$ & конечное множество правил вывода;\\
    
    $S$ & конечное множество аксиом (в отличие от нестохастических
    структурных моделей $|S| \geqslant 1$);\\
    
    $\Pi_S$ & вероятностная мера $p \in \Pi_S \in [0,1]$;\\
    
    $\pi_S$ & отображение, сопоставляющее каждому элементу $A_i \in S$
    его вероятностную меру $p \in \Pi_S$: $\pi_S \colon S \to \Pi_S$;\\
    
    $\Pi_R$ & вероятностная мера $p \in \Pi_R \in [0,1]$ (аналогично
    $\Pi_S$);\\
    
    $\pi_R$ & отображение, сопоставляющее каждому правилу подстановки
    $r \in R$ его вероятностную меру $p \in \Pi_R$, $\pi_R \colon R \to
    \Pi_R$.\\
  \end{tabular}
\end{defin}

Такое определение СГ является наиболее общим.

\begin{rem}
  Обычно рассматривают СГ с $|S| = 1$. В этом случае получается СГ:
  \begin{equation}
    \label{eq:Gs}
    G_S = \langle S, N, T, R, \Pi_R, \pi_R \rangle\,,
  \end{equation}
  т.\,к. $\pi_S \colon \langle A_i, 1 \rangle$, где $A_i$~--- аксиома.
\end{rem}

Как видно из определения, в состав каждой СГ входит известная нам
четвёрка $\langle N, T, R, S \rangle$, определённая ранее как
формальная грамматика. Такая ФГ получила специальное название:

\begin{defin}
  Формальная грамматика, лежащая в основе СГ $G_S$ \eqref{eq:Gs},
  т.\,е. СГ без вероятностей правил перехода, называется
  \emph{характеристической грамматикой} $G_S^0$ для данной СГ $G_S$:
  \begin{equation}
    \label{eq:Gs0}
    G_S^0 = \langle N, T, R, S \rangle\,.
  \end{equation}
\end{defin}

\begin{rem}
  Поскольку для каждой СГ существует её характеристическая, то все СГ
  можно классифицировать по типам:
  \begin{enumerate}
  \item СГ типа 0;
  \item СГ типа 1;
  \item СГ типа 2;
  \item СГ типа 3.
  \end{enumerate}
\end{rem}

\begin{ex}
  $G_S = \langle T, N, S, R, \pi_R \rangle$:
  \begin{center}
    \begin{tabular}{l}
      $T = \{a\}$,\\
      $N = \{S\}$,\\
      $S$,\\
      $R = \{S \xrightarrow{0{,}8} aSa,\; S \xrightarrow{0{,}2} aa\}$.\\
    \end{tabular}
  \end{center}
\end{ex}

\emph{Обозначение}. Правила СГ будем обозначать (для СГ типа 2 и 3):
\begin{equation}
  \label{eq:rj}
  r_j \colon A_j \xrightarrow{P_j} \beta_k \text{ или } A_j
  \xrightarrow{P_{i_k}} \beta_k\,,
\end{equation}
где\begin{tabular}[t]{r@{\;---\;}l@{}}
  $A_i \in N$ & левая часть правила;\\
  $\beta_k \in (T \cup N)^*$ & правая часть правил;\\
  $P_j$ или $P_{i_k}$ & вероятность применения правила $r_j \colon P_j
  = P_{i_k} = P(r_j)$.
\end{tabular}

Так же, как каждая ФГ $G$ порождает некоторый язык $L(G)$, так и
каждая СГ $G_S$ порождает стохастический язык $L(G_S) = L_S$.

Характерной особенностью СЯ является то, что каждое слово $\alpha \in
L_S$ СЯ имеет свою вероятностную меру $P(\alpha)$. Это равносильно
тому, что терминальный вывод цепочек $\alpha$ из аксиомы грамматики,
$S$ имеет вероятность:
$$P(S \stackrel[G_S]{*}{\Rightarrow} \alpha) = P(\alpha)\,.$$
Причём верно, что
$$
\sum_{\alpha \in L_S} P(\alpha) = 1\,,
$$
т.\,е. сумма вероятностей всех слов СЯ $L_S$ равна единице.

Это означает, что если при анализе ТМИ каждое событие предоставляется
некоторым словом $\alpha \in L_S$, то весь СЯ $L(G_S)$ образует
\emph{полную группу событий}.

\begin{defin}
  СГ $G_S$, порождающая СЯ:
  $$L(G_S) = \{ \langle \alpha, P(\alpha) \rangle \mid \alpha \in T^k,
  P(\alpha) = P(S \stackrel[G_S^0]{*}{\Rightarrow} \alpha) \}$$
  такой, что
  $$
  \sum_{\alpha \in L_S} P(\alpha) = 1\,,
  $$
  называется \emph{согласованной СГ}
\end{defin}

\begin{ex}
  $G_S \cdot R_S = \{ S \xrightarrow{0{,}8} aSa, S \xrightarrow{0{,}2}
  aa \}$
  
  $\alpha = aaaaaa$; $P(\alpha)$~--- ?
  
  \begin{figure}[h]
    \Tree [.S a [.S a [.S a a ] a ] a ]
    \caption{Дерево вывода}
  \end{figure}

  $S \stackrel{0{,}8}{\Rightarrow} aSa \xrightarrow{0{,}8}{\Rightarrow}
  aaSaa \stackrel{0{,}2}{\Rightarrow} aaaaaa$

  $P(\alpha) = P(aaaaaa) = 0{,}8 \cdot 0{,}8 \cdot 0{,}2 = 0{,}128$.

Определим согласованность СГ $G_S$:
\begin{equation*}
  \begin{split}
    \sum_{\alpha \in L(G_S)} P(\alpha) &= 0{,}2 + 0{,}2 \cdot 0{,}8 +
    0{,}2 \cdot 0{,}8 \cdot 0{,}8 + \ldots = \sum_{n=0}^{\infty} 0{,}2
    \cdot 0{,}8^n = \\
    &= 0{,}2 \cdot \sum_{n=0}^{\infty}0{,}8 = 0{,}2 \cdot
    \frac{1}{1-0{,}8} = 0{,}2 \frac{1}{0{,}2} = 1.
  \end{split}
\end{equation*}
\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Стохастические грамматики и вероятностные автоматы}

Формальные грамматики типа 3, как известно, эквивалентны по своей
мощности КА: т.\,е. языки, порождаемые ФГ типа 3 и КА~--- эквиваленты.

Аналогичная связь существует между СГ типа 3 и вероятностными
(стохастическими) конечными автоматами (\emph{СА}). Рассмотрим эту
аналогию.

Вспомним определением СА (вероятностного):
\begin{defin}
  СА называется кортеж (пятёрка)
  $$
  S_S = \langle X, Q, \pi_0, \delta_S, Q_F \rangle\,.
  $$
  
  \begin{tabular}[t]{r@{\;---\;}p{.85\textwidth}@{}}
    $X$ & входной алфавит СА;\\
    $Q$ & алфавит внутренних состояний СА;\\
    $\pi_0$ & вектор-строка (начальное распределение состояний;\\
    $Q_F$ & множество финальных состояний, $Q_F \subseteq Q$;\\
    $\delta_S$ & стохастическая (вероятностная) функция переходов.\\
  \end{tabular}
\end{defin}

\begin{ex}
  \label{ex:KA}  
  Для сравнения приведём вначале нестохастический КА.
\begin{itemize}
\item \emph{нестохастический КА} (задан графом переходов) 
  $$
  \boxed{\boxed{\delta \colon X \times Q \to Q}}
  $$
  \begin{figure}[h]
    \centering
    \begin{equation*}
      \entrymodifiers={+++[o][F-]}
      \SelectTips{cm}{}
      \xymatrix @+1pc {
        1 \ar[r]^a 
        & 2 \ar[r]^b \ar@/_25pt/[rr]^b \ar`ur^l/12pt[]`^dr[] _a []
        & 3 \ar[r]^b \ar`ur^l[]`^dr[] _b [] % \ar@(ru,lu)[]_b
        & *+++[o][F=]{\#}
      }
    \end{equation*}
    
    $$
    M =
    \begin{tabular}{c | c c c c}
     & 1 & 2 & 3 & $\#$\\\hline
     1 & & $a$\\
     2 & & $a$ & $b$ & $b$\\
     3 & & & $b$ & $b$\\
     $\#$\\
    \end{tabular}
    \qquad L(S) = \{a^n b^m \mid n,m \geqslant 1\}
    $$
    \caption{Нестохастический конечный автомат}
    \label{fig:KA}
  \end{figure}
\item стохастический КА (задан 3-хмерной автоматной матрицей) 
  $$
  \boxed{\boxed{\delta_S \colon X \times Q \times Q \to P}}
  $$

  $$
    M = \;\begin{tabular}{@{}c@{}}
      \begin{picture}(160,120)(-15,-15)
        \multiput(0,0)(20,0){5}%
        {\line(0,1){80}}
        \multiput(0,0)(0,20){5}%
        {\line(1,0){80}}
        
        \multiput(0,80)(12,4){6}%
        {\line(1,0){80}}
        \multiput(0,80)(20,0){5}%
        {\line(3,1){60}}
        
        \multiput(80,0)(0,20){5}%
        {\line(3,1){60}}
        \multiput(80,0)(12,4){6}%
        {\line(0,1){80}}
        
        \put(-15,68){$q_1$}
        \put(-15,48){$q_2$}
        \put(-15,28){$q_3$}
        \put(-10,5){$\vdots$}
        
        \put(7,-10){$q_1$}
        \put(27,-10){$q_2$}
        \put(47,-10){$q_3$}
        \put(63,-10){$\cdots$}

        \put(84,-8){$x_1$}
        \put(96,-4){$x_2$}
        \put(108,0){$x_3$}
        \multiput(123,6)(6,2){3}%
        {.}
      \end{picture}
    \end{tabular}
    $$

    $$
    M = \;\begin{tabular}{@{}c@{}}
      \begin{picture}(120,105)(-15,-15)
        \multiput(0,0)(20,0){5}%
        {\line(0,1){80}}
        \multiput(0,0)(0,20){5}%
        {\line(1,0){80}}
        
        \multiput(0,80)(12,4){3}%
        {\line(1,0){80}}
        \multiput(0,80)(20,0){5}%
        {\line(3,1){24}}

        \multiput(80,0)(0,20){5}%
        {\line(3,1){24}}
        \multiput(80,0)(12,4){3}%
        {\line(0,1){80}}
        
        \put(-12,66){$1$}
        \put(-12,46){$2$}
        \put(-12,26){$3$}
        \put(-13,6){$\#$}

        \put(7,-12){$1$}
        \put(27,-12){$2$}
        \put(47,-12){$3$}
        \put(65,-12){$\#$}

        \put(84,-8){$a$}
        \put(96,-4){$b$}
      \end{picture}
    \end{tabular}
    $$
  \begin{figure}[h]
    \centering
    $$
    M(a) = \begin{tabular}{c | c c c c}
      & 1 & 2 & 3 & $\#$\\\hline
      1 & & 1\\
      2 & & 0{,}4\\
      3 \\
      $\#$\\
    \end{tabular}
    \qquad M(b) = \begin{tabular}{c | c c c c}
      & 1 & 2 & 3 & $\#$\\\hline
      1\\
      2 & & & 0{,}5 & 0{,}1\\
      3 & & & 0{,}6 & 0{,}4\\
      $\#$\\
    \end{tabular}
    $$
    \caption{Стохастический конечный автомат}
    \label{fig:KAs}
  \end{figure}
  \begin{rem}
    Сумма элементов автоматной матрицы $M$ (3-хмерной) по слоям должна
    быть равна $1$.
  \end{rem}
\end{itemize}
\end{ex}

Соответствие между языком, допускаемым СА, и марковским процессом
устанавливается следующей теоремой.
\begin{theorem}
  Любой однородный марковский процесс с дискретным временем и конечным
  множеством состояний может быть представлен как язык, допускаемый
  \emph{стохастическим конечным автоматом}, т.\,е. если известен
  $X(t)$~--- марковский процесс, то от него можно перейти к СА $S_S$
  такому, что $L(S_S) = X(t)$.
\end{theorem}

Имеет место также следующая теорема, устанавливающая связь между СА и
СГ типа 3.

\begin{theorem}
  Для любого стохастического конечного автомата (СА) $S_S$ существует
  такая СГ типа 3, $G_S$, что:
  $$L(S_S) = L(G_S)$$
  и наоборот.
\end{theorem}

\emph{Алгоритм постоения $G_S$ типа 3 по СА $S_S$}:

$$S_S = \langle X, Q, \delta_S, Q_F \rangle\,,\qquad G_S = \langle T,
N, S, R \rangle\,,$$ тогда:
\begin{list}{\arabic{N})}{\usecounter{N}}
\item $T = X$;
\item $N = Q \mathbin{\backslash} \{\#\} = Q \mathbin{\backslash}
  Q_F$;
\item $[A_i \xrightarrow{P_{ijk}} x_j A_k]
  \Leftrightarrow [\delta(x_j,q_i,q_k) = p_{ijk}]$,\\
  $[A_i \xrightarrow{P_{ij}} x_j] \Leftrightarrow [\delta(x_j,q_i,\#)
  = p_{ij}]$;
\item $S = A_i = q_i$,\quad $S,A_i \in N$,\quad $q_i \in Q$,\quad
  $\pi_0 = \langle \pi_{0,1},\pi_{0,2},\ldots,\pi_{0,i},\ldots
  \rangle$,\quad $\pi_{0,i} = 1$.
\end{list}

\begin{ex}
  Построить $G_S$ по $S_S$ (см. пример на стр. \pageref{ex:KA})
  
  $G_S = \langle T, N, R_S, S \rangle$

  $T = \{a,b\}$,\quad $N = \{S, A, B\}$;\quad $q_1 \to S$, $q_2 \to
  A$, $q_3 \to B$.
  
  $R = \{\begin{tabular}[t]{@{} l @{} l}
    $S \xrightarrow{1} aA$,\\
    $A \xrightarrow{0{,}4} aA$,\\
    $A \xrightarrow{0{,}5} bB$,\\
    $A \xrightarrow{0{,}1} b$,\\
    $B \xrightarrow{0{,}6} bB$,\\
    $B \xrightarrow{0{,}4} b$ & \}      
  \end{tabular}$
\end{ex}

\emph{Алгоритм определения $P(\alpha)$ для СА $S_S$}.  
$$L(S_S) = \{\langle \alpha, P(\alpha) \mid \alpha \in X^*,
\alpha=x_1,x_2,\ldots,x_m,\;P(\alpha) = \pi_0M(x_1)M(x_2) \ldots
M(x_m)\pi_F^T \}$$
где
\begin{tabular}[t]{l @{\;---\;} l @{}}
  $\pi_{0\langle n \rangle} = \langle \pi_{01}, \pi_{02}, \ldots,
  \pi_{0i}, \ldots, \pi_{0n} \rangle$ & начальное распределение
  состояний;\\
  $\pi_{F\langle n \rangle} = \langle \pi_{F1}, \pi_{F2}, \ldots,
  \pi_{Fj}, \ldots, \pi_{Fn} \rangle$ & конечное распределение
  состояний;\\
\end{tabular}\\
причём $\pi_{Fj} = 1$, если $q_j \in Q_F$.

\begin{ex}
  $S_S$ из примера на стр. \pageref{ex:KA}.
  \begin{list}{\arabic{N})}{\usecounter{N}}
  \item $\alpha = aab$;\quad $\pi_{0\langle 4 \rangle} = \langle 1, 0,
    0, 0 \rangle$;\quad $\pi_{F\langle 4 \rangle} = \langle 0, 0, 0, 1
    \rangle$;
    \begin{gather*}
      P(aab) = \langle 1,0,0,0 \rangle
      \begin{bmatrix}
        0 & 1 & 0 & 0\\
        0 & 0{,}4 & 0 & 0\\
        0 & 0 & 0 & 0\\
        0 & 0 & 0 & 0\\
      \end{bmatrix}
      \begin{bmatrix}
        0 & 1 & 0 & 0\\
        0 & 0{,}4 & 0 & 0\\
        0 & 0 & 0 & 0\\
        0 & 0 & 0 & 0\\
      \end{bmatrix}
      \begin{bmatrix}
        0 & 1 & 0 & 0\\
        0 & 0 & 0{,}5 & 0{,}1\\
        0 & 0 & 0{,}6 & 0{,}4\\
        0 & 0 & 0 & 0\\
      \end{bmatrix}
      \begin{bmatrix}
        0\\
        0\\
        0\\
        1\\
      \end{bmatrix}
      =\\
      = \langle 0,1,0,0 \rangle
      \begin{bmatrix}
        0 & 1 & 0 & 0\\
        0 & 0{,}4 & 0 & 0\\
        0 & 0 & 0 & 0\\
        0 & 0 & 0 & 0\\
      \end{bmatrix}
      \begin{bmatrix}
        0 & 1 & 0 & 0\\
        0 & 0 & 0{,}5 & 0{,}1\\
        0 & 0 & 0{,}6 & 0{,}4\\
        0 & 0 & 0 & 0\\
      \end{bmatrix}
      \begin{bmatrix}
        0\\
        0\\
        0\\
        1\\
      \end{bmatrix} = \\
      = \langle 0,0{,}4,0,0 \rangle
      \begin{bmatrix}
        0 & 1 & 0 & 0\\
        0 & 0 & 0{,}5 & 0{,}1\\
        0 & 0 & 0{,}6 & 0{,}4\\
        0 & 0 & 0 & 0\\
      \end{bmatrix}
      \begin{bmatrix}
        0\\
        0\\
        0\\
        1\\
      \end{bmatrix} = \\
      = \langle 0,0,0{,}2,0{,}04 \rangle
      \begin{bmatrix}
        0\\
        0\\
        0\\
        1\\
      \end{bmatrix} = 0{,}04
    \end{gather*}
  \item Определим $P(\alpha)$ для $G_S \sim S_S$\,:
    $$S \stackrel{1}{\Rightarrow} aA \stackrel{0{,}4}{\Rightarrow} aaA
    \stackrel{0{,}1}{\Rightarrow} aab \Rightarrow P(\alpha) = 1 \cdot
    0{,}4 \cdot 0{,}1 = 0{,}04\,.$$
  \end{list}
\end{ex}

  \Tree[.S a [.A a [.A b ] ] ]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Восстановление (синтез) структурно-стохастических моделей (ССМ) функционирования БС}
\label{sec:synthesis}

Как указывалось при формулировке задач построения системы анализа ТМИ,
одной и центральных является проблема автоматического обучения системы
распознавания ТС.

При рассмотрении ССМ функционирования БС задача обучения системы
распознавания ТС формулируется как задача восстановления (синтеза) СГ.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Формулировка задачи восстановления СГ}

\begin{tabular}{@{} r @{\;---\;} p{.86\textwidth} @{}}
  $x_j$ & некоторый вычисляемый параметр ТС $x_j \in X$;\\
  $\bar{x}$ & значение параметра $\bar{x}$ в некоторый момент времени
  наблюдения распознавания;\\
  $\tilde{\bar{x}}$ & оценка значения параметра $x_j$, принимаемая по
  результатам его вычисления;\\
  $D_{\bar{x}_j}$ & дискретные значения, которые принимает параметр
  $x_j$ (область значений параметра $x_j$), $D_{\bar{x}_j}=
  \{a_{j1},a_{j2},\ldots,a_{jk},\ldots\}$;\\
  $|D_{\bar{x}_j}|$ & мощность (количество значений) параметра $\bar{x}_j$;\\
  $G_j$ & множество грамматик, каждая из которых порождает язык $L_j^k
  = L(G_j^k)$, соответствующий множеству значений тех параметров из
  $X$, с использованием которых вычисляется $\bar{x}_j$, $G_j =
  \{G_j^1,G_j^2,\ldots,G_j^k,\ldots\}$;\\ 
  $G_j^0$ & характеристическая грамматика соответствующей $G_j$.
\end{tabular}
\medskip

Для восстановления СГ $G_j^k$ используется отношение обучения такое,
что каждому языку $L_j^k$ (а значит, значению $\bar{x}_j = a_k \in
D_{\bar{x}_j}$) соответствует множество пар:
$$
\eta^k = \{\langle \alpha_i,m_i \rangle \mid \alpha_i \in L_j^k, \;
m_i\text{~--- кратность слова $\alpha$ в языке $L_j^k$}\}
$$

Тогда задача восстановления СГ $G_j^k$:

Дано:
\begin{tabular}[t]{l}
  $x_j \in X$,\\
  $a_{jk} \in D_{\bar{x}_j}$,\\
  $\eta^k = \{\langle \alpha_i,m_i \rangle\}$
\end{tabular}

Построить:
\begin{list}{\arabic{N})}{\usecounter{N}}
\item $G_j^0$~--- характеристическую грамматику;
\item $G_j^k$~--- стохастическую грамматику по значению $a_{jk}$.
\end{list}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Алгоритм синтеза СГ в условиях единственного
  значения $x_j$}

$$x_j: |D_{\bar{x}_j}| = 1$$

Рассмотрим СГ $G_S$ и обучающий язык $L_S$.
$$ L_S = \{\langle \alpha_l,m_l \rangle \}$$

$\hat{n}_{ij}(\alpha_l)$~--- случайная величина, равная числу
вхождений правила $A_i \to \beta_j$ в вывод цепочки $\alpha_l$ для
$G_S$.
$$ S \stackrel[G_S]{*}{\Rightarrow} \alpha_l$$

Оценка математического ожидания для $\hat{n}_{ij}(\alpha_l)$:
\begin{equation}
\tilde{M}[\hat{n}_{ij}] = \frac{\sum\limits_{\alpha_l \in L_S}m_l \cdot
  n_{ij}(\alpha_l)}{\sum\limits_{\alpha_l \in L_S}m_l}
\label{eq:M}
\end{equation}

В условиях согласованности СГ $G_S$ с $L_S$ оценка вероятности правил
подстановок:
\begin{equation}
  \tilde{P}_{ij} = \frac{\tilde{\bar{n}}_{ij}}{\sum\limits_{\langle i,j
      \rangle \in I_R^i}\tilde{\bar{n}}_{ij}} =
  \frac{\sum\limits_{\alpha_l \in L_S}m_l \cdot
    n_{ij}(\alpha_l)}{\sum\limits_{\langle i,j \rangle \in
      I_R^i}\sum\limits_{\alpha_l \in L_S}m_l \cdot n_{ij}(\alpha_l)}
\label{eq:Pij}
\end{equation}

где $I_R^i$~--- множество индексов правил в $R$ с левыми частями с $A_i$.

\begin{ex}
  См. пример на стр. \pageref{ex:KA}.

  $G_S^0\colon T = \{a,b\};\quad N = \{S,A,B\};\quad |L_S| = 100$
  \begin{center}
    \begin{tabular}{c l}
      1 & $S \xrightarrow{1} aA$,\\
      2 & $A \xrightarrow{0{,}4} aA$,\\
      3 & $A \xrightarrow{0{,}5} bB$,\\
      4 & $A \xrightarrow{0{,}1} b$,\\
      5 & $B \xrightarrow{0{,}6} bB$,\\
      6 & $B \xrightarrow{0{,}4} b$\\
    \end{tabular}
    \qquad
    \begin{tabular}{@{\quad} l @{\hspace{3em}} c @{\hspace{4em}} l @{\quad}}
      \hline\hline
      $\alpha_l$ & $m_l$ & $\{n_{ij}(\alpha_l)\}$\\\hline
      $a\ b\ b$ & 31 & 1,\ 3,\ 6\\
      $a\ b\ b\ b$ & 19 & 1,\ 3,\ 5,\ 6\\
      $a\ b$ & 15 & 1,\ 4\\
      $a\ a\ b\ b$ & 13 & 1,\ 2,\ 3,\ 6\\
      $a\ b\ b\ b\ b$ & 10 & 1,\ 3,\ 5,\ 5,\ 6\\
      $a\ a\ b$ & 6 & 1,\ 2,\ 4\\
      $a\ a\ a\ b\ b\ b$ & 3 & 1,\ 2,\ 2,\ 3,\ 5,\ 6\\
      $a\ a\ a\ b$ & 2 & 1,\ 2,\ 2,\ 4\\
      $a\ a\ a\ a\ b$ & 1 & 1,\ 2,\ 2,\ 2,\ 4\\\hline\hline
    \end{tabular}
  \end{center}

  \begin{center}
    \begin{tabular}{l l c c c}
      1 & $1\cdot(31 + 19 + 15 + 13 + 10 + 6 + 3 + 2 + 1)/100$ &
      $1$ & $1 / 1$ & $1{,}00$\\
      2 & $(1\cdot13 + 1\cdot6 + 2\cdot3 + 2\cdot2 +
      3\cdot1)/100$ & $0{,}32$ &  $0{,}32 / 1{,}32$ & $0{,}24$\\
      3 & $(1\cdot31 + 1\cdot19 + 1\cdot13 + 1\cdot10 + 1\cdot3)/100$
      & $0{,}76$ & $0{,}76/1{,}32$ & $0{,}58$\\
      4 & $(1\cdot15 + 1\cdot6 + 1\cdot2 + 1\cdot1)/100$ & $0{,}24$ &
      $0{,}24/1{,}32$ & $0{,}18$\\
      5 & $(1\cdot19 + 2\cdot10 + 1\cdot3)/100$ & $0{,}42$ &
      $0{,}42/1{,}18$ & $0{,}36$\\
      6 & $(1\cdot31 + 1\cdot19 + 1\cdot13 + 1\cdot10 + 1\cdot3)/100$
      & $0{,}76$ & $0{,}76/1{,}18$ & $0{,}64$\\
    \end{tabular}
  \end{center}
\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Алгоритм синтеза СГ в условиях множества значений $x_j$}
\label{sec:synthesis_multitude}

$$x_j \colon |D_{\bar{x}_j} > 1$$

\noindentИмеем:\\
$D_{\bar{x}} = \{a_1,a_2,\ldots,a_k,\ldots\}$,\\
$L_S = L_{S_1} \cup L_{S_2} \cup \ldots \cup L_{S_k} \cup \ldots$,\\
$G_S = \{G_{S_1}, G_{S_2}, \ldots, G_{S_k}, \ldots\}$\\
$P(\bar{x} = a_k) = P(a_k)$~--- априорная вероятность того, что
$\bar{x} = a_k$, причём $\sum\limits_k^{|D_{\bar{x}}|} P(a_k) = 1$.\\
$P(\alpha_l / \bar{x}_j = a_k) = P(\alpha_l / a_k)$~--- условная
вероятность цепочки $\alpha_l$ при условии, что имеет место значение
$\bar{x} = a_k$.

Теперь в выражении \eqref{eq:M} для $\tilde{\bar{n}}_{ij}$ разделим
каждое слагаемое числителя дроби на знаменатель:
\begin{equation*}
  \tilde{\bar{n}}_{ij}^k = \sum_{\alpha_l \in L_{S_k}}\left[
    \frac{m_l}{\sum_{\alpha_f in L_S} m_f} \right]
  \cdot \tilde{n}_{ij}^k (\alpha_l)
\end{equation*}

Заметим, что дробь в этом выражении есть $P(\alpha_l)$. Однако, в
условиях множества значений $\bar{x}$ эта вероятность зависит от
значения $\bar{x} = a_k$. Следовательно, эта дробь определяется как
вероятность зависимого события:
$$P(\alpha_l / a_k)\cdot P(a_k)$$
тогда выражения \eqref{eq:M} и \eqref{eq:Pij} принимают вид:

\begin{equation}
  \tilde{\bar{n}}_{ij}^k = \sum_{\alpha_l \in L_{S_k}} P(\alpha_l /
  a_k) \cdot P(a_k) \cdot \tilde{n}_{ij}^k (\alpha_l)
\label{eq:nijk}
\end{equation}

\begin{equation}
  \tilde{P}_{ij}^k =
  \frac{\tilde{\bar{n}}_{ij}^k}{\sum\limits_{\langle i,j\rangle \in
      I_R^i} \tilde{\bar{n}}_{ij}^k}
\label{eq:Pijk}
\end{equation}

В выражениях \eqref{eq:nijk} и \eqref{eq:Pijk}:
\begin{itemize}
\item $P(\alpha_l / a_k)$~--- может быть определена как кратность
  слова $\alpha_l$, в языке $L_{S_k}$, пронормированная по этому
  языку: $P(\alpha_l / a_k) = \frac{m_l^k}{\sum\limits_{\alpha_i \in
      L_{S_k}} m_i^k}$
\item $P(a_k)$~--- может быть задана экспертом или может быть равна:
  $P(a_k) = \frac{1}{|D_{\bar{x}}|}$ (в условиях равновероятных
  значений $\bar{x} = a_k$)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Рекуррентный алгоритм восстановления СГ}
\label{sec:recurrent_recovery_algorithm}

Реально при обучении системы распознавания ТС имеется, как правило,
рекуррентный процесс обучения, организованный по шагам. В таких
условиях имеется последовательность обучающих множеств:
\begin{equation*}
  L_S^1 \subset L_S^2 \subset L_S^3 \ldots \subset L_S^n \subset
  L_S^{n+1} \subset \ldots,
\end{equation*}
где $n$~--- шаг обучения.

Имеем оценки для $\tilde{\bar{n}}_{ij}^k$ и $\tilde{P}_{ij}^k$ на
$n$-ном шаге обучения $\tilde{\bar{n}}_{ij}^k \Bigr|_n$ и
$ \tilde{P}_{ij}^k \Bigr|_n$. Тогда для $(n+1)$-ого шага получим (с
учётом \eqref{eq:nijk} и \eqref{eq:Pijk}):
\begin{equation*}
  \tilde{\bar{n}}_{ij}^k \Bigr|_{n+1} = \tilde{\bar{n}}_{ij}^k
  \Bigr|_n + \sum_{\alpha_l \in L_S^{n+1} \mathop{\backslash} L_S^n}
  P(\alpha_l \mathop{/} a_k) \cdot P(a_k) \cdot \tilde{n}_{ij}^k
  (\alpha_l)
\end{equation*}
\begin{equation*}
   \tilde{P}_{ij}^k \Bigr|_{n+1} = \frac{\tilde{\bar{n}}_{ij}^k
     \Bigr|_{n+1}}{\sum_{\langle i,j \rangle \in I_R^i}
     \tilde{\bar{n}}_{ij}^k \Bigr|_{n+1}}
\end{equation*}

При обучении возможен такой случай, когда априорные вероятности
значений $\bar{x} = a_k$ меняются от шага к шагу. Рассмотрим, как
изменяются оценки для $\tilde P_{ij}$.

$P(a_k) \Bigr|_n$~--- вероятность значения $\bar{x} = a_k$ на $n$-ном
шаге обучения.

$L_S^{n+1} \mathop{\backslash} L_S^n = \alpha^*$~--- обучающее
множество увеличилось на одно слово.

Тогда выражения для $\tilde{\bar{n}}_{ij}^k \Bigr|_{n+1}$:
\begin{equation*}
  \tilde{\bar{n}}_{ij}^k \Bigr|_{n+1} = \tilde{\bar{n}}_{ij}^k
  \Bigr|_n + P(\alpha^* \mathop{/} a_k) \cdot P(a_k)\Bigr|_{n+1} \cdot
  (\alpha^*)
\end{equation*}

По формуле Байеса имеем: $P(a_k \mathop{/} \alpha^*) =
\frac{P(\alpha^* \mathop{/} a_k) P(a_k)}{\sum_{m=1}^{|D_{\bar{x}}|}
  P(\alpha^* \mathop{/} a_m) P(a_m)}$ $\Rightarrow$

\begin{equation}
  P(a_k)\Bigr|_{n+1} = \frac{P(\alpha^* \mathop{/} a_k)
    P(a_k)\Bigr|_n}{\sum\limits_{m=1}^{|D_{\bar{x}}|} P(\alpha^* \mathop{/}
    a_m) P(a_m)}
\end{equation}

Это выражение, использованное вместе с ранее полученным, даёт довольно
мощный и гибкий алгоритм восстановления СГ, который может быть с
успехом использован при обучении системы распознавания ТС БС.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Анализ информации (ТМИ) с использованием структурно-стохастических моделей (ССМ) функционирования БС}

В п.~\ref{sec:synthesis} мы рассмотрели методы восстановления ССМ с
использованием:
\begin{itemize}
\item характеристической грамматики, порождающей все возможные цепочки
  некоторого языка;
\item обучающего языка, являющегося подмножеством языка, порождаемого
  характеристической грамматикой.
\end{itemize}

Синтезированная ССМ в виде СГ позволяет с большой достоверностью
оценивать значения вычисляемых параметров ТС, а значит, и оценивать
сами ТС, в которых находится объект анализа.

Такое оценивание может быть произведено с помощью алгоритмов анализа
информации на основе \emph{анализа ССМ~--- СГ}.

В этом подразделе рассмотрим задачу анализа ТМИ с использованием СГ.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Формулировка задачи анализа ТМИ с использованием СГ}

Формальная постановка задачи анализа ТМИ формулируется с учётом задачи
синтеза СГ.

$x_j \in X$

$\bar{x}_j$~--- значение параметра $x_j$

$\tilde{\bar{x}}_j$~--- оценка значения параметра $x_j$

$D_{\bar{x}_j}= \{a_{j1},a_{j2},\ldots,a_{jk},\ldots\}$;

$|D_{\bar{x}_j}|$~--- мощность (количество значений) параметра
$\bar{x}_j$;

$G_j = \{G_j^1,G_j^2,\ldots,G_j^k,\ldots\}$~--- множество грамматик;

$\alpha^*$~--- некоторая входная цепочка в алфавите значений тех
параметров ТС $X$, с использованием которых определяется значение
параметра $x_j$ (или, в алфавите значений тех параметров ТС, от
которых зависит значение рассматриваемого параметра $x_j$.

Тогда задача анализа ТМИ, сводимая к задаче определения параметра ТС
$x_j$, формулируется как задача отнесения цепочек $\alpha^*$ к одному
из классов, каждый из которых задаётся множеством СГ $G_j$:

Дано: $\alpha^*$,\quad $G_j = \{G_{j1},G_{j2},\ldots,G_{jk},\ldots\}$.

Определить $G_{jm} \colon d_j^m = \max_{i \in I_{G_j}} \{d_j\}$,
т.\,е. определить ту СГ $G_{jm}$, что вероятность (мера)
принадлежности языку, порождаемому ею, будет максимальной.

Решение такой задачи проводится в два этапа:
\begin{description}
\item[на \emph{первом этапе}] проводится синтаксический анализ
  входного слова $\alpha^*$ для каждой СГ, соответствующей каждому
  значению параметра, т.,е. $x_j$.
\item[на \emph{втором этапе}] вычисляется апостериорная вероятность
  $P(a_{jk} \mathop{/} \alpha^*)$ для всех $k \in I_{G_j}$ и
  принимается решение о присвоении оценки значения параметра,
  т.,е. $\tilde{\bar{x}}_j$ той величине $a_{jm}$, которая обладает
  максимальной достоверностю, т.,е.
  \begin{equation*}
    a_{jm} = \arg \max_{a_{jk} \in D_{\bar{x}_j}} \{ P(a_{jk}
    \mathop{/} \alpha^*)\}
  \end{equation*}
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Алгоритм анализа СГ в условиях единственности значения $x_j$}

\begin{equation*}
  |D_{xj}| = 1
\end{equation*}

Поскольку в случае одноэлементного множества значений параметров ТС,
когда $|D_{\bar{x}_j}| = 1$, мы имеем единственную СГ $G_j$, то
анализ входной цепочки $\alpha^*$ сводится к определению её
вероятности в СЯ $L(G_j^1)$, поскольку $P(a_1 \mathop{/} \alpha^*) =
P(\alpha^* \mathop{/} a_1) = P(\alpha^*)$.

Вероятность цепочки $\alpha^*$ определяется:
\begin{equation*}
  P(\alpha^*) = P(\alpha^*_V)\,,
\end{equation*}
где $\alpha_V^*$~--- слово вывода цепочки $\alpha^*$ в СГ $G_{j1}$.

\begin{rem}
  Слово вывода $\alpha_V^*$ цепочки $\alpha^*$ в ФГ $G$ определяется
  как слово в алфавите $I_R = \{r_1,r_2,\ldots\}$~--- номера правил
  вывода в $R$~--- множестве правил вывода
  \begin{equation*}
    \alpha_V^* = r_{j1}r_{j2}\ldots r_{jk_j}\,,
  \end{equation*}
  такое, что каждый элемент $\alpha_V^*$~--- есть номер того правила в
  $R$, которое применяется в выводе $S \stackrel[G]{*}{\Rightarrow}
  \alpha^*$, а порядок применения этих правил задаётся порядком
  следования $r_j$ в слове вывода $\alpha_V^*$.
\end{rem}

\begin{ex}[к замечанию]\ 

  \noindent$G \colon $ 

  $T = \{a,b\}$

  $N = \{S, A, B\}$
  
  $R = \{\begin{tabular}[t]{@{}c l @{} l}
    1 & $S \rightarrow aA$,\\
    2 & $A \rightarrow aA$,\\
    3 & $A \rightarrow bB$,\\
    4 & $A \rightarrow b$,\\
    5 & $B \rightarrow bB$,\\
    6 & $B \rightarrow b$ & \}      
  \end{tabular}$
  
  $I_R = \{1,2,3,4,5,6\}$.
  
  \begin{center}
    \begin{tabular}{@{\quad} l @{\qquad} l @{\quad}}
      \hline\hline
      $\alpha^*$ & $\alpha^*_V$\\\hline
      $a\ b$ & 1\ 4\\
      $a\ b\ b$ & 1\ 3\ 6\\
      $a\ a\ b\ b$ & 1\ 2\ 3\ 6\\\hline\hline
    \end{tabular}
  \end{center}
\end{ex}

Итак, вероятность цепочки $\alpha^*$ в СЯ $L(G_{jk})$ есть
\begin{equation*}
  \begin{split}
    P(\alpha^*) = P(\alpha^*_V) &= P(r_{j1}\ldots r_{jk_j})=\\
    &= P(r_{j1})P(r_{j2})\ldots P(r_{jk_j}) =\\
    &= \prod_{r_j \in \alpha_V^*} P(r_j).
  \end{split}
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Алгоритм анализа СГ в условиях множества значений
  $x_j$}
\label{sec:analysis_single}


\begin{equation*}
  |D_{\bar{x}j}| = 1
\end{equation*}

В этом случае мы имеем множество СГ
\begin{equation*}
  G = \{G_1, G_2, \ldots, G_k, \ldots\}
\end{equation*}
(здесь и далее не будем указывать $j$).

Поэтому после проведения синтаксического анализа (грамматического
разбора) слова $\alpha^*$ по всем заданным характеристическим
грамматикам $G_k^0$, соответствующим СГ $G_k \in G$.

Далее проводится вычисление апостериорных вероятностей:
\begin{equation*}
  P(a_k \mathop{/} \alpha^*)
\end{equation*}
для всех $k \in I_G$ (по всему множеству $D_{\bar{x}j}$).

Для этого используется байесовский подход, в рамках которого:
\begin{equation*}
  P(a_k \mathop{/} \alpha^* = \frac{P(\alpha^* \mathop{/} a_k) P(a_k)}
  {\sum\limits_{i = 1}^{|D_{\bar{x}j}|} P(\alpha^* \mathop{/} a_i) P(a_i)}\,,
\end{equation*}
где \begin{tabular}[t]{@{} r @{\;---\;} p{.8\textwidth} @{}}
  $P(\alpha^* \mathop{/} a_k)$ & условные вероянтости принадлежности
  слова $\alpha^*$ языку $L(G_k)$, определяемые как в
  п.~\ref{sec:analysis_single};\\
  $P(a_k)$ & априорные вероятности $P(\bar{x}j = a_k)$ такие, что
  значение $\bar{x}j$ равно $a_k \in D_{\bar{x}j}$.
\end{tabular}

Величина $P(a_k)$, как и в случае восстановления СГ
(см. п.~\ref{sec:synthesis_multitude}) может быть задана экспертом, а
в условиях максимальной неопределённости~---
\begin{equation*}
  P(a_k) = \frac 1 {|D_{\bar{x}j}|}\text{~--- для $k = 1(1)
    \left|D_{\bar{x}j}\right|$}
\end{equation*}

\begin{ex}
  
\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Рекуррентный алгоритм анализа СГ}

По аналогии с алгоритмом рекуррентного восстановления СГ
(см. п.~\ref{sec:recurrent_recovery_algorithm}), процесс анализа
входной цепочки в наиболее эффективном виде реализуется
рекуррентно~--- в процессе поступления символов входной цепочки.

\begin{tabular}[t]{@{} r @{\;---\;} p{.815\textwidth} @{}}
  $P(a_k)\bigr|_n$ & вероятность значения $\bar{x} = a_k$ на $n$-ном
  шаге анализа ($n = 0,1,2,\ldots$) (после анализа $n-1$ символов
  входной цепочки $\alpha^*$)\\
  $P(a_k)\bigr|_{n+1}$ & вероятность значения $\bar{x} = a_k$ на $n +
  1$-м шаге анализа (после получения $n$-ого символа цепочки
  $\alpha^*$.
\end{tabular}

Тогда $P(a_k)$ на последующем шаге определится через значение $P(a_k)$
на предыдущем шаге по следующей формуле:
\begin{equation*}
  P(a_k)\bigr|_{n+1} = \frac{P(\alpha^* \mathop{/} a_k) P(a_k)\bigr|_n}
  {\sum\limits_{i=1}^{|D_{\bar{x}j}|} P(\alpha^* \mathop{/} a_i) P(a_i)\bigr|_n}
\end{equation*}
где \begin{tabular}[t]{@{} r @{\;---\;} p{.8\textwidth} @{}}
  $P(\alpha^* \mathop{/} a_k)$ & определяется для каждого значения
  $a_k$ или, соответственно, для каждой СГ $G_k \in G$ через
  вероятность слова вывода.
\end{tabular}







%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "lections"
%%% End: 
